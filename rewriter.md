# Awesome resources about intent understanding component in the RAG system

ðŸ”¥ **Must-read papers for intent understanding component in RAG.**

ðŸŒŸ **We greatly appreciate any contributions via PRs, issues, emails, or other methods.**


## Introduction

The intent understanding component is to understand the question, and guide the retrieval to obtain better documents. Usually, there could be different types of understanding, e.g., whether to retrieve and query formulations.


## Table of Content (ToC)

- [Retrieval Detection](#retrieval_detect)
- [Query Reformulation](#query_reformulate)
- [Query Expansion](#retrieval_expansion)
  - [Generative-Relevance Feedback](#GRF)
  - [Pseudo-Relevant Feedback](#PRF)
  - [Methods Combination](#methods_combination)



## 1. Retrieval Detection <a id="retrieval_detect"></a>

| Date       | Title                                                                                                           | Authors                                  | Orgnization                                                                                                   | Abs                                                                                             |
|------------|-----------------------------------------------------------------------------------------------------------------|------------------------------------------|---------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|
|2024/05/04| [When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively](https://arxiv.org/pdf/2404.19705) <br>[[code](https://github.com/mwozgpt/Adapt-LLM-anonymous-old): ![](https://img.shields.io/github/stars/mwozgpt/Adapt-LLM-anonymous-old.svg?style=social)] | Tiziano Labruna, et al. | University of Bozen-Bolzano | <small>This paper presents ADAPT-LLM by fine-tuning a base LLM on an open-domain QA dataset. It first take base LLM to zero-shot evaluation to determin its accuracy in QA. For questions with incorrect answers, it train the LLM to generate a spectial token <RET>, indicating the need for additional context.</small>|
|2024/02/15| [Grounding Language Model with Chunking-Free In-Context Retrieval](https://arxiv.org/abs/2402.09760) | Hongjin Qian, et al. | Gaoling School of Artificial Intelligence, Renmin University of China, | <small>This paper presents a novel Chunking-Free In-Context (CFIC) retrieval approach, specifically tailored for Retrieval-Augmented Generation (RAG) systems. CFIC addresses these challenges by circumventing the conventional chunking process. It utilizes the encoded hidden states of documents for in-context retrieval, employing auto-aggressive decoding to accurately identify the specific evidence text required for user queries, eliminating the need for chunking. CFIC is further enhanced by incorporating two decoding strategies, namely Constrained Sentence Prefix Decoding and Skip Decoding. These strategies not only improve the efficiency of the retrieval process but also ensure that the fidelity of the generated grounding text evidence is maintained.</small>|
|2023/10/08| [Self-Knowledge Guided Retrieval Augmentation for Large Language Models](https://arxiv.org/pdf/2310.05002.pdf) |Yile Wang, Peng Li, Maosong Sun, Yang Liu|Tsinghua University| <small>This work introduces Self-Knowledge guided Retrieval augmentation*SKR* to flexibly call the retriever. Three steps: 1) collection self-knowledge of LLM by asking a number of questions, and divide the question into two categories D+ and D- according to the answer correctness, 2) eliciting self-knowledge of LLMs by either direct prompt or training a classifier based on D+ and D-, 3) using self-knowledge for adaptive retrieval augmentation based on prediction of 2).</small>|

## 2. Query Reformulation <a id="query_reformulate"></a>

| Date       | Title                                                                                                           | Authors                                  | Orgnization                                                                                                   | Abs                                                                                             |
|------------|-----------------------------------------------------------------------------------------------------------------|------------------------------------------|---------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|
| 2024/03/31 | [RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation](https://arxiv.org/pdf/2404.00610.pdf)| Chi-Min Chan, Chunpu Xu, Ruibin Yuan, et. al. |Hong Kong University of Science and Technology, Hong Kong Polytechnic University, MIT|<small>This work proposes RQ-RAG (Refine Query RAG) to enhance the generator (LLaMA2) to explicitly rewrite, decompose, and disambiguate, before final answer generation. In this way, the RAG process interleaves between retrieval (guided by refined query) and generation.  </small> |

## 3. Query Expansion <a id="retrieval_expansion"></a>

### 3.1 Generative-Relevance Feedback <a id="GRF"></a>
| Date       | Title                                                                                                                                                                                                                                                                      | Authors                                                                        | Orgnization                                                                                                                                                                                                                                          | Abs                                                                                                                                                                                                                                                                                                                                                                 |
|------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 2024/08/24 | [Meta Knowledge for Retrieval Augmented Large Language Models](https://arxiv.org/abs/2408.09017) | Laurent Mombaerts, Terry Ding, Florian Felice, Jonathan Taws, Adi Banerjee, Tarik Borogovac |  Amazon Web Services | <small>The work proposes a novel data-centric RAG workflow for LLMs, relying on generating metadata and synthetic Questions and Answers (QA) for each document, as well as introducing the new concept of Meta Knowledge Summary (MK Summary) for metadata-based clusters of documents. It transforms the traditional retrieve-then-read system into a more advanced prepare-then-rewrite-then-retrieve-then-read framework, to achieve higher domain expert-level understanding of the knowledge base.</small> |
| 2023/10/19 | [Large Language Models Know Your Contextual Search Intent: A Prompting Framework for Conversational Search](https://arxiv.org/abs/2303.06573), <br>[[code](https://github.com/kyriemao/LLM4CS): ![](https://img.shields.io/github/stars/kyriemao/LLM4CS.svg?style=social)] | Kelong Mao, Zhicheng Dou, Fengran Mo, Jiewen Hou, Haonan Chen, Hongjin Qian    | Gaoling School of Artificial Intelligence Renmin University of China,Engineering Research Center of Next-Generation Search and Recommendation MOE,UniversitÃ© de MontrÃ©al QuÃ©bec Canada,Institute of Computing Technology Chinese Academy of Sciences | <small>The work proposes a simple yet effective prompting framework, called **LLM4CS**, to leverage LLMs as a text-based search intent interpreter to help conversational search.It explores three prompting methods to generate multiple query rewrites and hypothetical responses, and then proposes to aggregate them into an integrated representation.</small> |
| 2023/10/11 | [Query2doc: Query Expansion with Large Language Models](https://arxiv.org/abs/2303.07678), <br>[[code](https://github.com/PKUnlp-icler/PCA-EVAL): ![](https://img.shields.io/github/stars/PKUnlp-icler/PCA-EVAL.svg?style=social)]                                         | Liang Wang, Nan Yang, Furu Wei                                                 | Microsoft Research                                                                                                                                                                                                                                   | <small>This work proposes a simple yet effective query expansion approach, denoted as **Query2doc**, to improve both sparse and dense retrieval systems. The proposed method first generates pseudo-documents by few-shot prompting large language models (LLMs), and then expands the query with generated pseudo-documents.</small>                               |
| 2023/6/16  | [GRM: Generative Relevance Modeling Using Relevance-Aware Sample Estimation for Document Retrieval](https://arxiv.org/abs/2306.09938)                                                                                                                                      | Iain Mackie, Ivan Sekulic, Shubham Chatterjee, Jeffrey Dalton, Fabio Crestani. | University of Glasgow,UniversitÃ  della Svizzera italiana                                                                                                                                                                                             | <small>This work proposes Generative Relevance Modeling **(GRM)** that uses Relevance-Aware Sample Estimation (RASE) for more accurate weighting of expansion terms. Specifically, it identifies similar real documents for each generated document and uses a neural re-ranker to estimate their relevance.</small>                                                |
|2022/12/20| [Precise Zero-Shot Dense Retrieval without Relevance Labels](https://arxiv.org/abs/2212.10496),<br> [[code](https://github.com/texttron/hyde): ![](https://img.shields.io/github/stars/texttron/hyde.svg?style=social)|Luyu Gao, Xueguang Ma, Jimmy Lin, Jamie Callan.|Language Technologies Institute, Carnegie Mellon University,David R. Cheriton School of Computer Science, University of Waterloo| <small>This work proposes to pivot through Hypothetical Document Embeddings(HyDE), which first zero-shot instructs an instruction-following language model to generate a hypothetical document and then grounds the generated document to the actual corpus with an unsupervised contrastively learned encoder.</small>|

### 3.2 Pseudo-Relevant Feedback <a id="PRF"></a>
| Date       | Title                                                                                                                                                                                                                                                                      | Authors                                                                        | Orgnization                                                                                                                                                                                                                                          | Abs                                                                                                                                                                                                                                                                                                                                                                 |
|------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|2023/8/2| [Large Language Models are Strong Zero-Shot Retriever](https://arxiv.org/abs/2304.14233)|Tao Shen, Guodong Long, Xiubo Geng, Chongyang Tao, Tianyi Zhou, Daxin Jiang.|AAII, Microsoft,University of Maryland| <small>This work proposes the Language language model as Retriever **(LameR)** to augment a query with its potential answers by prompting LLMs with a composition of the query and the queryâ€™s in-domain candidates and proposes to leverage a non-parametric lexicon-based method (e.g., BM25) as the retrieval module to capture query-document overlap in a literal fashion.</small>|

### 3.3 Methods Combination <a id="methods_combination"></a>
| Date       | Title                                                                                                                                                                                                                                                                      | Authors                                                                        | Orgnization                                                                                                                                                                                                                                          | Abs                                                                                                                                                                                                                                                                                                                                                                |
|------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|2023/12/12| [Synergistic Interplay between Search and Large Language Models for Information Retrieval](https://arxiv.org/abs/2305.07402)] ,<br>[[code](https://github.com/Cyril-JZ/InteR): ![](https://img.shields.io/github/stars/Cyril-JZ/InteR.svg?style=social)]|Tao Shen, Guodong Long, Xiubo Geng, Chongyang Tao, Tianyi Zhou, Daxin Jiang.|Peking University, Microsoft,AAII| <small>This work proposes **InteR**, a novel framework that facilitates information refinement through synergy between RMs and LLMs,which allows RMs to expand knowledge in queries using LLM-generated knowledge collections and enables LLMs to enhance prompt formulation using retrieved documents.</small>|
|2023/5/12| [Generative and Pseudo-Relevant Feedback for Sparse, Dense and Learned Sparse Retrieval](https://arxiv.org/abs/2305.07477)|Iain Mackie, Shubham Chatterjee, Jeffrey Dalton.|University of Glasgow| <small>This work proposes combining generative and pseudo-relevance feedback ranking signals to achieve the benefits of both feedback classes.</small>|
|2023/5/5| [Query Expansion by Prompting Large Language Models](https://arxiv.org/abs/2305.03653)|LRolf Jagerman, Honglei Zhuang, Zhen Qin, Xuanhui Wang, Michael Bendersky.|Google Research| <small>This work proposes an approach to query expansion that leverages the generative abilities of Large Language Models (LLMs) and studies a variety of different prompts, including zero-shot, few-shot and Chain-of-Thought (CoT) finding that CoT prompts are especially useful for query expansion.</small>|
