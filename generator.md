# Awesome resources about rank component in the RAG system

üî• **Must-read papers for generate component in RAG.**

üèÉ **Coming soon: Add one-sentence intro to each paper.**

üåü **We greatly appreciate any contributions via PRs, issues, emails, or other methods.**


## Introduction

Coming soon ...


## Table of Content (ToC)


- [Methods](#methods)
  - [Sub-title 1](#subtitle1)
  - [Sub-title 2](#subtitle2)
- [Datasets](#datasets)
  - [Sub-title 1](#1-sub-rerank)
  - [Sub-title 2](#2-sub-rerank)
- [Metrics](#metrics)
  - [Claim Verification](#claimverification)
  - [Sub-title 2](#2-sub-compressor)





## 1. Methods <a id="methods"></a>

### 1.1 Sub-title 1


- [2023/10] **This is the paper Title** *Authors et al. arXiv.* [[paper](https://arxiv.org/abs/2310.02071)] [[code](https://github.com/PKUnlp-icler/PCA-EVAL), ![](https://img.shields.io/github/stars/Tongji-KGLLM/RAG-Survey.svg?style=social)]
  - This work proposes XXXXX, which benchmarks document ranking via XXX methods and XXX methods from XXXX, XXX, and XXXX......

## 2. Datasets <a id="datasets"></a>

### 2.1 Sub-title 1


- [2022/04/12] **ASQA: Factoid Questions Meet Long-Form Answers** *Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, Ming-Wei Chang. arXiv.* [[paper](https://arxiv.org/abs/2204.06092)] [[dataset](https://huggingface.co/datasets/din0s/asqa)]
  - ASQA is the first long-form question answering dataset that focuses on ambiguous factoid questions.
- [2024/01/26] **Benchmarking Large Language Models in Complex Question Answering Attribution using Knowledge Graphs** *Nan Hu, Jiaoyan Chen, Yike Wu, Guilin Qi, Sheng Bi, Tongtong Wu, Jeff Z. Pan. arXiv.* [[paper](https://arxiv.org/abs/2401.14640)]
  - CAQA is a new benchmark for complex question answering attribution, which is designed to evaluate the ability of LLMs to answer complex questions with the help of knowledge graphs.

## 3. Metrics <a id="metrics"></a>

### 3.1 Claim Verification <a id="claimverification"></a>

- [2024/02/23] **Merging Facts, Crafting Fallacies: Evaluating the Contradictory Nature of Aggregated Factual Claims in Long-Form Generations** *Cheng-Han Chiang, Hung-yi Lee. arXiv.* [[paper](https://arxiv.org/abs/2402.05629#)]
  - This work finds that combining factual claims together can result in a non-factual paragraph due to entity ambiguity. Current metrics for fact verification fail to properly evaluate these non-factual passages. The authors proposed D-FActScore based on FActScore, and showed the methods and results of human and automatic evaluation.  