# Awesome resources about answer verification component in the RAG system

üî• **Must-read papers for answer verification component in RAG.**

üèÉ **Coming soon: Add one-sentence intro to each paper.**

üåü **We greatly appreciate any contributions via PRs, issues, emails, or other methods.**


## Introduction

The answer generated by LLMs are tend to be haullucinations. To overcome this, it would be better to introduce an additional verification component to check the generated answer.


## Table of Content (ToC)


- [Reasoning With CoT](#cot)


## 1. CoT-based (Reasoning) Methods <a id="cot"></a>

| Date       | Title                                                                                                           | Authors                                  | Orgnization                                                                                                   | Abs                                                                                             |
|------------|-----------------------------------------------------------------------------------------------------------------|------------------------------------------|---------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|
|2023/12/31| [Rethinking with Retrieval: Faithful Large Language Model Inference](https://arxiv.org/abs/2301.00303.pdf)] <br> [[code](https://github.com/HornHehhf/RR): ![](https://img.shields.io/github/stars/HornHehhf/RR.svg?style=social) |Hangfeng He, Hongming Zhang, Dan Roth|University of Rochester, Tencent AI Lab Seattle, University of Pennsylvania | <small>This work propose a novel post-processing approach, rethinking with retrieval (RR), which uses decomposed reasoning steps obtained from CoT prompting to retrieve relevant docs for LLMs. Four steps: 1)CoT prompting to generate explanation E and prediction P for query Q. 2)Sampling diverse reasoning path R (i.e., E + P), 3)knowledge K retrieval for each path, 4)faithful inference (NLI model) for each R+K.</small>|