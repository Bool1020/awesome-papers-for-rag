# Awesome resources about answer enhancement component in the RAG system

üî• **Must-read papers for answer enhancement component in RAG.**

üèÉ **Coming soon: Add one-sentence intro to each paper.**

üåü **We greatly appreciate any contributions via PRs, issues, emails, or other methods.**


## Introduction

The answer generated by LLMs are tend to be haullucinations. To overcome this, it would be better to introduce an additional component to increase the quality of the answer, namely **answer enhancement** component. The


## Table of Content (ToC)

- [Answer Verification](#verify)
	- [Attribution Detection](#attribution)
	- [Claim Verification](#verification)
- [Reasoning-based (CoT) Filtering](#cot)


## 1. Answer Verifiaction <a id="verify"></a>

### 1.1 Attribution Detection <a id="attribution"></a>
| Date       | Title                                                                                                           | Authors                                  | Orgnization                                                                                                   | Abs                                                                                             |
|------------|-----------------------------------------------------------------------------------------------------------------|------------------------------------------|---------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|
|2023/10/07| [Automatic Evaluation of Attribution by Large Language Models](https://arxiv.org/pdf/2305.06311.pdf) <br> [[code](https://github.com/OSU-NLP-Group/AttrScore): ![](https://img.shields.io/github/stars/OSU-NLP-Group/AttrScore.svg?style=social) |Xiang Yue, Boshi Wang, Ziru Chen, et. al.|The Ohio State University | <details><summary><small>This paper presents evaluation...</small></summary><small>This work tries to evaluate the attribution ability (3 types: attributable, extrapolatory, contradictory) of existing LLMs by introducing two benchmarks (i.e., AttrEval-Simulation and AttrEval-GenSearch). It also introduces two types of automatic evaluation methods: 1) Prompting LLMs, 2) Fine-tuning LMs on Repurposed Data. </small></details>|

### 1.2 Claim Verification <a id="verification"></a>
| Date       | Title                                                                                                           | Authors                                  | Orgnization                                                                                                   | Abs                                                                                             |
|------------|-----------------------------------------------------------------------------------------------------------------|------------------------------------------|---------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|
|2024/07/02| [Pelican: Correcting Hallucination in Vision-LLMs via Claim Decomposition and Program of Thought Verification](https://arxiv.org/pdf/2407.02352.pdf) <br> [[code]() |Pritish Sahu, Karan Sikka, Ajay Divakaran|SRI International, Princeton| <details><summary><small>This paper presents Pelican ...</small></summary><small>Pelican 1) decomposes the visual claim into a chain of sub-claims based on first-order predicates, 2) it then use Program-of-Thought prompting to generate Python code for answering these questions through flexible composition of external tools. </small></details>|
|2024/02/23 |[Merging Facts, Crafting Fallacies: Evaluating the Contradictory Nature of Aggregated Factual Claims in Long-Form Generations](https://arxiv.org/abs/2402.05629.pdf)| heng-Han Chiang, Hung-yi Lee.|National Taiwan University|<details><summary><small>This paper presents D-FActScore ...</small></summary><small>This work finds that combining factual claims together can result in a non-factual paragraph due to entity ambiguity. Current metrics for fact verification fail to properly evaluate these non-factual passages. The authors proposed D-FActScore based on FActScore, and showed the methods and results of human and automatic evaluation.</small></details>|
|2023/10/20| [Explainable Claim Verification via Knowledge-Grounded Reasoning with Large Language Models](https://arxiv.org/abs/2310.05253.pdf) <br> [[code](https://github.com/wang2226/FOLK): ![](https://img.shields.io/github/stars/wang2226/FOLK.svg?style=social) |Haoran Wang, Kai Shu|Illinois Institute of Technology, Chicago| <details><summary><small>This paper presents FOLK ...</small></summary><small>This work introduces First-Order-Logic-Guided Knowledge-Grounded (**FOLK**). 1ÔºâFOLK translates input claim into a FOL clause and uses it to guide LLMs to generate a set of question-answer pairs, 2) FOLK then retrieves knowledge-grounded answers from external knowledge-source; 3) FOLK performs FOL-guided reasoning over knowledge-grounded answers to make veracity prediction and generate explanations.</small></details>|

## 2. Reasoning-based (CoT) Filtering <a id="cot"></a>

| Date       | Title                                                                                                           | Authors                                  | Orgnization                                                                                                   | Abs                                                                                             |
|------------|-----------------------------------------------------------------------------------------------------------------|------------------------------------------|---------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|
|2023/12/31| [Rethinking with Retrieval: Faithful Large Language Model Inference](https://arxiv.org/abs/2301.00303.pdf) <br> [[code](https://github.com/HornHehhf/RR): ![](https://img.shields.io/github/stars/HornHehhf/RR.svg?style=social) |Hangfeng He, Hongming Zhang, Dan Roth|University of Rochester, Tencent AI Lab Seattle, University of Pennsylvania | <details><summary><small>This paper presents RR ...</small></summary><small>This work propose a novel post-processing approach, rethinking with retrieval (RR), which uses decomposed reasoning steps obtained from CoT prompting to retrieve relevant docs for LLMs. Four steps: 1)CoT prompting to generate explanation E and prediction P for query Q. 2)Sampling diverse reasoning path R (i.e., E + P), 3)knowledge K retrieval for each path, 4)faithful inference (NLI model) for each R+K.</small></details>|